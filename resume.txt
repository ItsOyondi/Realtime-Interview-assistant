
Lead Data Scientist/ GenAI Specialist

Profile Summary:

With over 13 years of experience in IT and 11 years in Data Science, I possess deep expertise in artificial intelligence, data mining, deep learning, predictive analysis, and machine learning, particularly with large datasets comprising both structured and unstructured data. I am adept at managing the entire data science project lifecycle and have actively contributed to all phases of project execution.

I have specialized knowledge in handling exponentially growing data, applying my understanding of the underlying science of data to a diverse array of problem statements across various domains.
Automated recurring reports using SQL and Python, visualizing the results on BI platforms such as Tableau.

Experienced in implementing stored procedures, triggers, and functions using T-SQL.

Skilled in developing a fully automated, highly elastic cloud orchestration framework on AWS.

Proficient in predictive modeling, data mining methods, factor analysis, ANOVA, hypothesis testing, normal distribution, and other advanced statistical and econometric techniques.

Expertise in Tableau and R-Shiny for data visualization, creating interactive reports and dashboards that deliver actionable insights from large datasets.

Experienced in developing and analyzing data models, writing both simple and complex SQL queries to extract data from databases for analysis and testing.

Proficient in statistical programming languages, including R, Python, and SAS, and familiar with cloud platforms like Azure ML, AWS ML, and GCP tools such as BigQuery and Vertex AI.

Experienced in using CUDA/GPU API for real-time image processing.

Strong SQL programming skills, particularly in working with functions, packages, and triggers.

Developed predictive models using decision trees, random forests, Naïve Bayes, logistic regression, cluster analysis, and neural networks.

Well-versed in all phases of the software development lifecycle (SDLC) and familiar with Agile and Scrum methodologies.

Knowledgeable in ML techniques and algorithms, including k-NN, Naive Bayes, SVM, and natural language processing (NLP).

Technical Skills:

Data Science Specialties:  Natural Language Processing, Machine Learning, Internet of Things (IoT), Predictive Analytical thinking, Predictive Maintenance
Analytical Skills: Bayesian Analysis, Inference, Time-Series Analysis, Regression Analysis, Linear Models, Multivariate Analysis, Gradient Descent, Sampling methods, Forecasting, Segmentation, Clustering, Sentiment Analysis, and Predictive Analytics. Experience in stochastic optimization and regression with machine learning algorithms, formulating and solving discrete and continuous optimization problems.  
Analytics Tools: Classification and Regression Trees (CART), Support Vector Machine (SVM), Random Forest, Gradient Boosting Machine (GBM), Principal Component Analysis (PCA), Naïve Bayes,
Analytic Languages and Scripts:  Python, Spark, Spark SQL, Scala, Java, C++, Ruby, LaTeX, Markup
Data Integration: SQL Server Integration Services (SSIS), SQL Server Reporting Services (SSRS)
Languages: C, C++, R, Java, Perl, SQL, Python
Version Control:  Git Hub, Git, SVN
Libraries: ggplot, nltk, numpy, opencv, pandas, sci-kit-learn, scipy, spacy, multiprocessing, dask, flask
Computing Environments: Jupyter, Spyder IDE, Atom, VBCode, Eclipse, Android Studio
Command Shell: iPython, Mac OS X, Powershell, Bash
Data Query:  Azure, Google, SQL and NoSQL, various SQL and NoSQL databases, and data warehouses. Experience with AWS, GCP, Azure cloud computing, Spark, and Tableau.
Deep Learning: Machine perception, Data Mining, Machine Learning algorithms, Neural Networks, Tensor Flow, Keras
Soft Skills:  Able to deliver presentations and highly technical reports; collaborating with stakeholders and cross-functional teams, advisement on how to leverage analytical insights.  Development of clear analytical reports which directly address strategic goals.
Operating Systems:  Windows 8, 10, and 11, and Linux Ubuntu


Professional Experience:


Lead Data Scientist/ GenAI Specialist 						March 2023 – Present
Devoted Health, Eagan, MN

As a Senior Data Scientist and Generative AI Specialist at Devoted Health, I played a crucial role in developing predictive models focused on member engagement metrics using KNN similarity modeling. I led project management efforts to streamline model development, leveraging advanced SQL and PySpark for efficient data processing and compliance assessment with Medicare health measures. By conducting thorough reviews of existing models, I identified optimization opportunities and enhanced member segmentation strategies. Additionally, I pioneered the use of generative models to create synthetic data, improving our predictive capabilities while ensuring data privacy. My collaborative approach fostered effective communication within cross-functional teams, enabling the successful integration of generative AI into our workflows.

Played a pivotal role in the initial design of models as a Senior Data Scientist, focusing on engagement metrics as targets and utilizing KNN similarity modeling to enhance the accuracy and effectiveness of predictions regarding customer behavior and outcomes.
Demonstrated project management skills by scheduling and overseeing meetings to facilitate model development and identify relevant data tables that aligned with specific business objectives.
Conducted a comprehensive review of previous models written in Python to understand the underlying logic and insights, identifying opportunities for enhancement and optimization in future iterations.
Applied advanced SQL techniques and complex queries to assess member eligibility and compliance with Medicare health measures within specific contracts and plans, enabling accurate assessments and improved decision-making processes.
Utilized PySpark to distribute calculations and execute queries on large datasets, optimizing performance and enabling efficient data processing.
Coordinated with the team to address various ad-hoc requests and provided assistance with Python and PySpark for research studies related to Medicare measure lift.
Contributed significantly to feature selection by leveraging PTC models and employing Random Forests/XGBoost to predict the likelihood of members returning a Home-Lab-Kit, thereby supporting targeted marketing initiatives and enhancing member engagement.
Documented weekly meeting discussions and provided support using Teams, ensuring effective communication and knowledge sharing within the team.
Provided consultation and expertise in model versioning and refinement based on monthly feedback and evaluations of predicted member lists.
Designed and implemented hold-out groups to test model predictions regarding program compliance, contributing to the assessment and enhancement of program effectiveness.
Developed refined categorizations of historical compliance and engagement, incorporating various combinations of factors influencing Medicare categorizations for improved accuracy in member segmentation.
Leveraged KNN similarity factors and engagement scores to assign Medicare measures to active members lacking historical records, enabling targeted interventions and personalized care plans.
Analyzed data across different levels of granularity, including member, plan, and contract levels, to extract actionable insights and drive strategic decision-making.
Employed Natural Language Processing (NLP) techniques for preprocessing textual data, conducting data cleaning and processing to ensure data quality and consistency.
Managed and processed vast volumes of data with billions of rows and thousands of high-dimensional features, demonstrating expertise in handling complex datasets.
Led the development of a recommendation system for plan-level members, utilizing PowerShell SSH in a Hadoop/Hive environment and employing Impala for complex multiple joins.
Used Jupyter Notebooks to execute models and implemented SHAP (SHapley Additive exPlanations) to evaluate the marginal effects and interpret the predictability of members returning a Home-Lab-Kit.
Developed and fine-tuned generative models to create synthetic data for training purposes, enhancing the robustness of predictive models while preserving data privacy.
Conducted experiments with Generative Adversarial Networks (GANs) to generate realistic data samples for various scenarios, improving the performance of machine learning models.
Implemented prompt engineering techniques to optimize responses from large language models, ensuring high-quality outputs for customer interactions and automated reporting.
Collaborated with cross-functional teams to integrate generative AI capabilities into existing workflows, enhancing the efficiency of data processing and insights generation.
Evaluated and monitored the performance of generative AI models, providing insights and recommendations for continuous improvement and alignment with business objectives.


Sr. Data Science Consultant							Jan 2022 – Mar 2023
Conoco Phillips, Houston, TX

As a Senior Data Scientist Consultant at ConocoPhillips, I facilitated collaboration between the Lighthouse team and data vendors to integrate new data pipelines and provide model preparation consultations. I managed diverse datasets spanning satellite imagery, financial data, and environmental metrics, ensuring compliance with data usage regulations. Additionally, I developed Python code for data ingestion and transformation, optimized data pipelines for Snowflake, and leveraged GCP BigQuery for big data insights, enhancing operational efficiency and data-driven decision-making across projects.

Acted as a liaison between the Lighthouse team and data vendors, integrating new pipelines and offering consultations for model preparation.
Conducted weekly meetings with vendors and the gamma team to review JIRA-assigned tickets.
Gained experience across diverse domains with varying granularity and frequencies, including satellite imagery, foot traffic, credit card transactions, international device usage, social listening, financial data, demographics, telecom, GIS, geospatial data, ESG ratings, global temperatures, vegetation, and air quality.
Evaluated the legality of data usage across raw, derived, and model-inferred data types.
Managed vendor staging of data and machine learning APIs utilizing vendor-specific platforms like GCP, AWS, and Azure.
Provided feature updates to repositories to enhance pipelines directed to Snowflake and debugged airflow event issues.
Utilized APIs, SFTP, S3, and GCS for managing vendor-specific source data staging, employing dynamic SQL to create queries across hundreds of Snowflake databases, schemas, tables, and views.
Developed source code in Python across three large Git repositories to facilitate data ingestion, transformation, validation, final staging, and markup for client portals and internal projects.
Leveraged GCP BigQuery to support vendors and case teams with big data and model-inferred insights using Vertex AI.
Implemented Python virtual classes and documented data lineage and pipeline details in Egnyte, Confluence, and SharePoint.
Oversaw pipeline scheduling and conducted log reviews of DAGs coded with ECS operator Python structures.
Converted protocols from REST APIs to GraphQL for ingesting geospatial maritime vendor data.
Cloned and tested repositories using locally run Docker containers before pushing changes upon successful verification.
Employed regex for dynamic pattern matching of S3 directories with parameterized queries based on current dates or other incoming events.
Developed enumeration modules to identify changes in vendor source data staging structures.
Participated in weekly JIRA meetings to discuss ongoing projects and vendor data proposals for new integrations.




Sr. Data Scientist        	 							Oct 2021 – Jan 2022
Credit Suisse, New York City, NY

As a Senior Data Scientist at Credit Suisse, I spearheaded projects focused on Regulatory Anomaly Detection and Tax data Extraction. I collaborated with international clients and coordinated with our IT teams to develop complex SQL queries and build robust data pipelines, enhancing our data processing capabilities. By implementing advanced machine learning techniques and Optical Character Recognition (OCR), I improved the accuracy of tax form data extraction to 99%, while leveraging tools like Python and TensorFlow for comprehensive data analysis and visualization. My efforts facilitated real-time insights and streamlined operations, contributing significantly to regulatory compliance and operational efficiency.

Contributed to two distinct projects: Regulatory Anomaly Detection (RAD) and Tax Extraction.
Engaged in daily scrum meetings using Odyssey and Atlassian JIRA to create tasks and establish priorities.
Collaborated weekly with European clients and coordinated with the IT data center in India during off-hours to build complex SQL queries processed by Impala engines for error trading data.
Orchestrated multiple pipelines with cron jobs to ingest and process error trading data into Hive, facilitating connections to Tableau for aggregated visualizations based on regulatory requirements, jurisdictions, and data sources.
Developed anomaly detection models using isolated random forests, leveraging features extracted from stocks and derivatives error trading data.
Implemented Optical Character Recognition (OCR) using Tesseract and Convolutional Neural Networks (CNNs).
Processed the extracted text with Natural Language Processing (NLP), applying word embeddings with BERT and utilizing Long Short-Term Memory (LSTM) networks for topic modeling.
Utilized deep learning frameworks such as PyTorch and Keras within TensorFlow.
Employed tools including Python, NumPy, SciPy, and Pandas for data analysis.
Acted as the lead developer, creating object-oriented Python classes to process PDF tax forms stored in MongoDB, achieving a 99% accuracy rate in data extraction with Tesseract.
Used synthetic training data in TensorFlow to model variations in handwritten dates on tax forms.
Created JSON schemas for specific coordinate extractions using OpenCV for tax forms such as W9, EXP, ECI, W8BENE, WBEN, and IMY.
Applied homographic coordinate alignment and morphological transformations to images utilizing OpenCV.
Employed regular expressions to post-process OCR-extracted text data, filtering out noise.
Implemented multi-threading and processing techniques in Python to manage multiple pages within a single tax form, significantly reducing data processing and extraction time.


Sr. Data Scientist									Dec 2019 – Oct 2021
Cleveland-Cliffs, Cleveland, OH

As a Senior Data Scientist at Cleveland-Cliffs, I ingested and analyzed IoT data from diverse sources, including HVR, SAP, and APIs, to develop robust data pipelines with Kafka and Snowpipe. I ensured data persistence through solutions like Blob Storage and Snowflake, and utilized Azure Streaming Data and Databricks for analytics, effectively visualizing insights with Power BI. My role involved remote testing of predictive models at the Peru Mine and collaborating with cross-functional teams to implement innovative solutions that enhanced operational efficiency and decision-making.


Ingested IoT data using various sources, including HVR, SAP, PIBA, and APIs.
Developed data pipelines utilizing Kafka, Event Hubs, and Snowpipe.
Ensured data persistence through Blob Storage (Cosmos), Teradata, and Snowflake.
Conducted data analytics using Azure Streaming Data, Python, Azure Databricks, SQL, GIT, and Azure.
Leveraged Power BI to visualize data stored in Cosmos in JSON format.
Utilized diverse IoT data types, including GPS, temperature, pressure, altitude, speed, and bumps.
Performed Databricks cluster analysis and runtime tests to identify latency issues.
Developed REST API and web socket applications to remotely control mining operations, utilizing Postman for testing.
Organized complex data relationships using the brainstorming tool FreeMind.
Participated in daily stand-up meetings to discuss tasks assigned in Azure DevOps, following Agile methodologies.
Executed operations sequentially or in parallel using Pandas or Spark DataFrames.
Conducted remote testing of model recommendations with clients in real-time at the Peru Mine in South America.
Analyzed the duration of various downtime events in the mine.
Engineered features from IoT data to measure or trigger events in the mining operations.
Researched and documented past and current DevOps items in a wiki.
Led discussions with clients on adopting the proposed model for use in the mine.
Developed predictive methods for estimating when shovel material blocks would be depleted.
Evaluated the accuracy of mine KPI estimates derived from Monte Carlo simulations.
Analyzed code for model production and experimented with Python classes and data structure extraction to enhance understanding and suggest core code changes.
Established JDBC connections to data sources using Databricks.
Collaborated with a core team of 4-6 data engineers, data analysts, and data scientists.



EDUCATION

Bachelor of Science in Computer Science
Master of Science in Data Science & Analytics
Grand Valley State University